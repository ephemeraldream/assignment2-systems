{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5ecc6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs336_systems.benchmarking import benchmarking_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8da942a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'forward_time_total': 0.2829209230003471,\n",
       " 'forward_time_per_step': 0.014146046150017355,\n",
       " 'forward_backward_time_total': 0.9212588479999795,\n",
       " 'forward_backward_time_per_step': 0.046062942399998974,\n",
       " 'backward_time_per_step': 0.03191689624998162,\n",
       " 'steps': 20,\n",
       " 'warmup_steps': 5,\n",
       " 'batch_size': 4,\n",
       " 'seq_len': 256,\n",
       " 'device': 'cuda',\n",
       " 'model_params': 41556480}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    \n",
    "benchmarking_script(\n",
    "        num_layers=6,\n",
    "        d_model=512,\n",
    "        num_heads=8,\n",
    "        d_ff=2048,\n",
    "        context_length=1024,\n",
    "        vocab_size=32000,\n",
    "        batch_size=4,\n",
    "        seq_len=256,\n",
    "        device=\"cuda\" if __import__(\"torch\").cuda.is_available() else \"cpu\",\n",
    "        warmup_steps=5,\n",
    "        num_steps=20,\n",
    "        rope_theta=10000.0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66694f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'forward_time_total': 0.5235735200003546,\n",
       " 'forward_time_per_step': 0.02617867600001773,\n",
       " 'forward_backward_time_total': 0.9545965919996888,\n",
       " 'forward_backward_time_per_step': 0.04772982959998444,\n",
       " 'backward_time_per_step': 0.021551153599966708,\n",
       " 'steps': 20,\n",
       " 'warmup_steps': 0,\n",
       " 'batch_size': 4,\n",
       " 'seq_len': 256,\n",
       " 'device': 'cuda',\n",
       " 'model_params': 41556480}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmarking_script(\n",
    "        num_layers=6,\n",
    "        d_model=512,\n",
    "        num_heads=8,\n",
    "        d_ff=2048,\n",
    "        context_length=1024,\n",
    "        vocab_size=32000,\n",
    "        batch_size=4,\n",
    "        seq_len=256,\n",
    "        device=\"cuda\" if __import__(\"torch\").cuda.is_available() else \"cpu\",\n",
    "        warmup_steps=0,\n",
    "        num_steps=20,\n",
    "        rope_theta=10000.0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2f26bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'forward_time_total': 0.3061218689999805,\n",
       " 'forward_time_per_step': 0.015306093449999026,\n",
       " 'forward_backward_time_total': 0.9062522469998839,\n",
       " 'forward_backward_time_per_step': 0.045312612349994195,\n",
       " 'backward_time_per_step': 0.03000651889999517,\n",
       " 'steps': 20,\n",
       " 'warmup_steps': 2,\n",
       " 'batch_size': 4,\n",
       " 'seq_len': 256,\n",
       " 'device': 'cuda',\n",
       " 'model_params': 41556480}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmarking_script(\n",
    "        num_layers=6,\n",
    "        d_model=512,\n",
    "        num_heads=8,\n",
    "        d_ff=2048,\n",
    "        context_length=1024,\n",
    "        vocab_size=32000,\n",
    "        batch_size=4,\n",
    "        device=\"cuda\" if __import__(\"torch\").cuda.is_available() else \"cpu\",\n",
    "        warmup_steps=2,\n",
    "        num_steps=20,\n",
    "        rope_theta=10000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7c1bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, 10, bias=False)\n",
    "        self.ln = nn.LayerNorm(10)\n",
    "        self.fc2 = nn.Linear(10, out_features, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "     x = self.relu(self.fc1(x))\n",
    "     x = self.ln(x)\n",
    "     x = self.fc2(x)\n",
    "     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "522b7885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.float32, torch.float32, torch.float32, torch.float32]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(4, 10)  \n",
    "toy_model = ToyModel(10, 10)\n",
    "\n",
    "output = toy_model(x)\n",
    "[param.dtype for param in toy_model.parameters()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "470cef15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model parameters (should be float32):\n",
      "[torch.float32, torch.float32, torch.float32, torch.float32]\n",
      "\n",
      "Using autocast for mixed precision:\n",
      "Model parameters during autocast (unchanged):\n",
      "[torch.float32, torch.float32, torch.float32, torch.float32]\n",
      "Output tensor dtype: torch.float16\n",
      "First feed-forward layer output dtype during autocast:\n",
      "  fc1 output: torch.float16\n",
      "  relu output: torch.float16\n",
      "  layer norm output: torch.float32\n",
      "\n",
      "Predicted logits analysis:\n",
      "Output shape: torch.Size([4, 10])\n",
      "Output range: [-1.0049, 1.1182]\n",
      "\n",
      "Loss computation:\n",
      "Loss dtype: torch.float16\n",
      "Loss value: -4.7617\n",
      "\n",
      "Gradients analysis:\n",
      "Gradient dtypes:\n",
      "  fc1.weight: param=torch.float32, grad=torch.float32\n",
      "  ln.weight: param=torch.float32, grad=torch.float32\n",
      "  ln.bias: param=torch.float32, grad=torch.float32\n",
      "  fc2.weight: param=torch.float32, grad=torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Demonstrating autocast behavior with FP16 mixed precision\n",
    "print(\"Original model parameters (should be float32):\")\n",
    "print([param.dtype for param in toy_model.parameters()])\n",
    "\n",
    "# Move model and input to CUDA for autocast\n",
    "toy_model = toy_model.cuda()\n",
    "x = x.cuda()\n",
    "\n",
    "print(\"\\nUsing autocast for mixed precision:\")\n",
    "with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "    y = toy_model(x)\n",
    "    print(\"Model parameters during autocast (unchanged):\")\n",
    "    print([param.dtype for param in toy_model.parameters()])\n",
    "    print(f\"Output tensor dtype: {y.dtype}\")\n",
    "    print(f\"First feed-forward layer output dtype during autocast:\")\n",
    "    with torch.no_grad():\n",
    "        x_fc1 = toy_model.fc1(x)\n",
    "        print(f\"  fc1 output: {x_fc1.dtype}\")\n",
    "        x_relu = toy_model.relu(x_fc1)\n",
    "        print(f\"  relu output: {x_relu.dtype}\")\n",
    "        x_ln = toy_model.ln(x_relu)\n",
    "        print(f\"  layer norm output: {x_ln.dtype}\")\n",
    "\n",
    "print(\"\\nPredicted logits analysis:\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(f\"Output range: [{y.min().item():.4f}, {y.max().item():.4f}]\")\n",
    "\n",
    "print(\"\\nLoss computation:\")\n",
    "loss = y.sum()\n",
    "print(f\"Loss dtype: {loss.dtype}\")\n",
    "print(f\"Loss value: {loss.item():.4f}\")\n",
    "\n",
    "print(\"\\nGradients analysis:\")\n",
    "loss.backward()\n",
    "print(\"Gradient dtypes:\")\n",
    "for name, param in toy_model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"  {name}: param={param.dtype}, grad={param.grad.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e861bf4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 32.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "s = torch.tensor(0,dtype=torch.float16)\n",
    "for i in range(100000):\n",
    "    s += torch.tensor(0.01,dtype=torch.float16)\n",
    "end_time = time.time()\n",
    "print(f\"Result: {s}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
