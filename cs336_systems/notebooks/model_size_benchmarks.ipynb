{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Size Benchmarking with Memory Profiling\n",
        "\n",
        "\n",
        "## –†–∞–∑–º–µ—Ä—ã –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:\n",
        "- **small**: d_model=768, d_ff=3072, num_layers=12, num_heads=12\n",
        "- **medium**: d_model=1024, d_ff=4096, num_layers=24, num_heads=16  \n",
        "- **large**: d_model=1280, d_ff=5120, num_layers=36, num_heads=20\n",
        "- **xl**: d_model=1600, d_ff=6400, num_layers=48, num_heads=25\n",
        "- **2.7B**: d_model=2560, d_ff=10240, num_layers=32, num_heads=32\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n",
            "GPU Memory: 8.6 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from cs336_systems.benchmarking import benchmarking_script\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π –∑–∞–≥—Ä—É–∂–µ–Ω—ã:\n",
            "small: {'d_model': 768, 'd_ff': 3072, 'num_layers': 12, 'num_heads': 1}\n",
            "medium: {'d_model': 1024, 'd_ff': 4096, 'num_layers': 24, 'num_heads': 1}\n"
          ]
        }
      ],
      "source": [
        "model_configs = {\n",
        "    \"small\": {\n",
        "        \"d_model\": 768,\n",
        "        \"d_ff\": 3072,\n",
        "        \"num_layers\": 12,\n",
        "        \"num_heads\": 1\n",
        "    },\n",
        "    \"medium\": {\n",
        "        \"d_model\": 1024,\n",
        "        \"d_ff\": 4096,\n",
        "        \"num_layers\": 24,\n",
        "        \"num_heads\": 1\n",
        "    },\n",
        "    # \"large\": {\n",
        "    #     \"d_model\": 1280,\n",
        "    #     \"d_ff\": 5120,\n",
        "    #     \"num_layers\": 36,\n",
        "    #     \"num_heads\": 1\n",
        "    # },\n",
        "    # \"xl\": {\n",
        "    #     \"d_model\": 1600,\n",
        "    #     \"d_ff\": 6400,\n",
        "    #     \"num_layers\": 48,\n",
        "    #     \"num_heads\": 25\n",
        "    # },\n",
        "    # \"2.7B\": {\n",
        "    #     \"d_model\": 2560,\n",
        "    #     \"d_ff\": 10240,\n",
        "    #     \"num_layers\": 32,\n",
        "    #     \"num_heads\": 32\n",
        "    # }\n",
        "}\n",
        "common_params = {\n",
        "    \"context_length\": 128,\n",
        "    \"vocab_size\": 10000,\n",
        "    \"batch_size\": 2, \n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"warmup_steps\": 3,\n",
        "    \"num_steps\": 10,\n",
        "    \"rope_theta\": 10000.0,\n",
        "    \"profile_memory\": True\n",
        "}\n",
        "\n",
        "print(\"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π –∑–∞–≥—Ä—É–∂–µ–Ω—ã:\")\n",
        "for name, config in model_configs.items():\n",
        "    print(f\"{name}: {config}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üî• –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: SMALL\n",
            "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: d_model=768, layers=12, heads=1\n",
            "------------------------------------------------------------\n",
            "‚úÖ –ú–æ–¥–µ–ª—å small –∑–∞–≤–µ—Ä—à–µ–Ω–∞:\n",
            "   Forward: 23.85 ms/step\n",
            "   Backward: 32.27 ms/step\n",
            "   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: 120.9M\n",
            "   GPU Memory: 1.11 GB\n",
            "\n",
            "üî• –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: MEDIUM\n",
            "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: d_model=1024, layers=24, heads=1\n",
            "------------------------------------------------------------\n",
            "‚úÖ –ú–æ–¥–µ–ª—å medium –∑–∞–≤–µ—Ä—à–µ–Ω–∞:\n",
            "   Forward: 52.53 ms/step\n",
            "   Backward: 74.20 ms/step\n",
            "   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: 412.9M\n",
            "   GPU Memory: 3.47 GB\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "warmup_results = {}\n",
        "\n",
        "\n",
        "for model_name, config in model_configs.items():\n",
        "    print(f\"\\nüî• –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: {model_name.upper()}\")\n",
        "    print(f\"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: d_model={config['d_model']}, layers={config['num_layers']}, heads={config['num_heads']}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    try:\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            torch.cuda.reset_peak_memory_stats()\n",
        "        \n",
        "        results = benchmarking_script(\n",
        "            **config,\n",
        "            **common_params\n",
        "        )\n",
        "        \n",
        "        warmup_results[model_name] = results\n",
        "        \n",
        "        print(f\"‚úÖ –ú–æ–¥–µ–ª—å {model_name} –∑–∞–≤–µ—Ä—à–µ–Ω–∞:\")\n",
        "        print(f\"   Forward: {results['forward_time_per_step']*1000:.2f} ms/step\")\n",
        "        print(f\"   Backward: {results['backward_time_per_step']*1000:.2f} ms/step\")\n",
        "        print(f\"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {results['model_params']/1e6:.1f}M\")\n",
        "        \n",
        "        if 'memory_info' in results and results['memory_info']:\n",
        "            mem_info = results['memory_info']\n",
        "            print(f\"   GPU Memory: {mem_info['max_memory_allocated']:.2f} GB\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ {model_name}: {str(e)}\")\n",
        "        continue\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å torch.compile\n",
            "\n",
            "üî• –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: SMALL\n",
            "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: d_model=768, layers=12, heads=1\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0817 11:22:59.071715 335433 .venv/lib/python3.11/site-packages/torch/_inductor/config.py:613] compile_threads set to 1 via env\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mCanceled future for execute_request message before replies were done"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "print('–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å torch.compile')\n",
        "for model_name, config in model_configs.items():\n",
        "    print(f\"\\nüî• –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: {model_name.upper()}\")\n",
        "    print(f\"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: d_model={config['d_model']}, layers={config['num_layers']}, heads={config['num_heads']}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    try:\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            torch.cuda.reset_peak_memory_stats()\n",
        "        \n",
        "        results = benchmarking_script(\n",
        "            **config,\n",
        "            **common_params,\n",
        "            torch_compile=True\n",
        "        )\n",
        "        \n",
        "        warmup_results[model_name] = results\n",
        "        \n",
        "        print(f\"‚úÖ –ú–æ–¥–µ–ª—å {model_name} –∑–∞–≤–µ—Ä—à–µ–Ω–∞:\")\n",
        "        print(f\"   Forward: {results['forward_time_per_step']*1000:.2f} ms/step\")\n",
        "        print(f\"   Backward: {results['backward_time_per_step']*1000:.2f} ms/step\")\n",
        "        print(f\"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {results['model_params']/1e6:.1f}M\")\n",
        "        \n",
        "        if 'memory_info' in results and results['memory_info']:\n",
        "            mem_info = results['memory_info']\n",
        "            print(f\"   GPU Memory: {mem_info['max_memory_allocated']:.2f} GB\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ {model_name}: {str(e)}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "–°–æ–∑–¥–∞–¥–∏–º —Ç–∞–±–ª–∏—Ü—ã –∏ –≥—Ä–∞—Ñ–∏–∫–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
